{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df: pd.DataFrame):\n",
    "\n",
    "    df['ProductCode'], uniques = pd.factorize(df['ProductId'])\n",
    "    df['UserCode'], uniques = pd.factorize(df['UserId'])\n",
    "\n",
    "    df['SummaryLength'] = df['Summary'].str.split().str.len()\n",
    "    df['TextLength'] = df['Text'].str.split().str.len()\n",
    "\n",
    "    for col in ['ProductCode', 'UserCode', 'Time', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'SummaryLength', 'TextLength']:\n",
    "        df[col] = (df[col] - df[col].mean())/df[col].std()\n",
    "\n",
    "    return df.drop(columns=['ProductId', 'UserId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_process(df: pd.DataFrame):\n",
    "    df['Summary'] = df['Summary'].fillna('')\n",
    "    snow_stemmer = SnowballStemmer(language='english')\n",
    "    for i in range(len(df)):\n",
    "        if i%100000 == 0:\n",
    "            print(\"summary process: \", i)\n",
    "        words = []\n",
    "        split_summary = re.findall(r'\\w+', df.loc[i, 'Summary'])\n",
    "        for w in split_summary:\n",
    "            x = snow_stemmer.stem(w)\n",
    "            words.append(x)\n",
    "        df.at[i, 'Summary'] = ' '.join(words)\n",
    "    return df\n",
    "\n",
    "def summary_analysis(df: pd.DataFrame):\n",
    "    vectorizer = TfidfVectorizer(min_df=0.01, max_df=0.75)\n",
    "    tfidV_vectors = vectorizer.fit_transform(df['Summary'].to_numpy())\n",
    "    res_mat = []\n",
    "    for i in range(tfidV_vectors.shape[0]):\n",
    "        res_mat.append(np.asarray(tfidV_vectors[i].todense()).reshape(-1))\n",
    "    result_df = pd.DataFrame(res_mat, columns=vectorizer.get_feature_names_out())\n",
    "    return pd.concat([df, result_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(df: pd.DataFrame):\n",
    "    start = datetime.datetime.now()\n",
    "    df['Text'] = df['Text'].fillna('')\n",
    "    snow_stemmer = SnowballStemmer(language='english')\n",
    "    for i in range(len(df)):\n",
    "        if i%100000 == 0:\n",
    "            print(\"text process: \", i)\n",
    "        words = []\n",
    "        split_summary = re.findall(r'\\w+', df.loc[i, 'Text'])\n",
    "        for w in split_summary:\n",
    "            x = snow_stemmer.stem(w)\n",
    "            words.append(x)\n",
    "        df.at[i, 'Text'] = ' '.join(words)\n",
    "    end = datetime.datetime.now()\n",
    "    print(\"running time:\", end-start)\n",
    "    return df\n",
    "\n",
    "def text_analysis(df: pd.DataFrame):\n",
    "    print(\"text analysis:\")\n",
    "    start = datetime.datetime.now()\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidV_vectors = vectorizer.fit_transform(df['Text'].to_numpy())\n",
    "    res_mat = []\n",
    "    for i in range(tfidV_vectors.shape[0]):\n",
    "        res_mat.append(np.asarray(tfidV_vectors[i].todense()).reshape(-1))\n",
    "    result_df = pd.DataFrame(res_mat, columns=vectorizer.get_feature_names_out())\n",
    "    end = datetime.datetime.now()\n",
    "    print(\"running time:\", end-start)\n",
    "    return pd.concat([df, result_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train length:  1697533\n",
      "submission length:  300000\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "trainingSet = pd.read_csv(\"./data/train.csv\")\n",
    "# Load test set\n",
    "submissionSet = pd.read_csv(\"./data/test.csv\")\n",
    "print(\"train length: \", len(trainingSet))\n",
    "print(\"submission length: \", len(submissionSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text process:  0\n",
      "text process:  100000\n",
      "text process:  200000\n",
      "text process:  300000\n",
      "text process:  400000\n",
      "text process:  500000\n",
      "text process:  600000\n",
      "text process:  700000\n",
      "text process:  800000\n",
      "text process:  900000\n",
      "text process:  1000000\n",
      "text process:  1100000\n",
      "text process:  1200000\n",
      "text process:  1300000\n",
      "text process:  1400000\n",
      "text process:  1500000\n",
      "text process:  1600000\n",
      "running time: 0:39:03.905326\n"
     ]
    }
   ],
   "source": [
    "# trainingSet = summary_process(trainingSet)\n",
    "trainingSet = text_process(trainingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainingSet = summary_analysis(trainingSet)\n",
    "# trainingSet.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet = text_analysis(trainingSet)\n",
    "trainingSet.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the DataFrame\n",
    "train_processed = process(trainingSet)\n",
    "train_processed.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on Id so that the test set can have feature columns as well\n",
    "testX= pd.merge(train_processed, submissionSet, left_on='Id', right_on='Id')\n",
    "testX = testX.drop(columns=['Score_x'])\n",
    "testX = testX.rename(columns={'Score_y': 'Score'})\n",
    "\n",
    "# The training set is where the score is not null\n",
    "trainX =  train_processed[train_processed['Score'].notnull()]\n",
    "\n",
    "testX.to_csv(\"./data/X_test_text.csv\", index=False)\n",
    "trainX.to_csv(\"./data/X_train_text.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fba1db2ae0bf2a01e825a4782efeb28dbb746a1b089c66234dc73062bb30ad6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
